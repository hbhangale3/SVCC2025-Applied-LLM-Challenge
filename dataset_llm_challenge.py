# -*- coding: utf-8 -*-
"""Dataset_LLM_Challenge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R_tANPT7bJOTx6gwDhUxfiIM8zxIdTI4
"""

# --- Cell 1: Setup Kaggle API ---
import os

# Install Kaggle library
!pip install -q kaggle

# Upload kaggle.json
from google.colab import files
print("Upload your kaggle.json file:")
uploaded = files.upload()

# Check if upload was successful
if 'kaggle.json' in uploaded:
    print("kaggle.json uploaded successfully!")
    # Create directory and move the json file
    !mkdir -p ~/.kaggle
    !cp kaggle.json ~/.kaggle/
    !chmod 600 ~/.kaggle/kaggle.json
    print("Kaggle API configured.")
else:
    print("Upload failed or cancelled.")

# Clean up uploaded file variable if needed
# del uploaded

from google.colab import drive
drive.mount('/content/drive')

# --- Cell 3: Define Output Paths on Mounted Google Drive (Corrected) ---
import os
import pathlib

# Define the base path for your project on Google Drive
DRIVE_BASE_PATH = '/content/drive/MyDrive/'
# --- CORRECTION: Remove leading slash ---
PROJECT_FOLDER_NAME = 'NetLingo_project' # CHANGE THIS if you used a different folder name
# ---
DRIVE_PROJECT_PATH = os.path.join(DRIVE_BASE_PATH, PROJECT_FOLDER_NAME)

# Define where the final token text files will be saved ON GOOGLE DRIVE
DRIVE_TOKENS_DIR = pathlib.Path(DRIVE_PROJECT_PATH) / "data" / "tokens"

# Create the directories if they don't exist within your Drive
DRIVE_TOKENS_DIR.mkdir(parents=True, exist_ok=True)

# Define the full paths for the output files
OUT_KYOTO_DRIVE = DRIVE_TOKENS_DIR / "kyoto_tokens.txt"
OUT_CIC_DRIVE = DRIVE_TOKENS_DIR / "cic_tokens.txt"
OUT_UNSW_DRIVE = DRIVE_TOKENS_DIR / "unsw_tokens.txt"

print(f"Base project path on Drive: {DRIVE_PROJECT_PATH}")
print(f"Token output directory on Drive: {DRIVE_TOKENS_DIR}")
print(f"Kyoto output file path: {OUT_KYOTO_DRIVE}")
print(f"CIC-IDS output file path: {OUT_CIC_DRIVE}")
print(f"UNSW output file path: {OUT_UNSW_DRIVE}")

# --- Cell 4: Download Datasets DIRECTLY to Google Drive (with Dataset Progress Bar) ---
import os
import pathlib
import zipfile
from tqdm.notebook import tqdm # Import tqdm

# Ensure Drive is mounted (should have been done in Cell 2)
if not os.path.exists('/content/drive/MyDrive'):
    print("❗️ Google Drive is not mounted. Please run the mounting cell first.")
else:
    print("Google Drive appears mounted.")

    # Define base project path on Drive (from Cell 2/3)
    DRIVE_PROJECT_PATH = '/content/drive/MyDrive/NetLingo_project' # Make sure this is correct

    # Define where raw data will be stored ON GOOGLE DRIVE
    DRIVE_RAW_DATA_DIR = pathlib.Path(DRIVE_PROJECT_PATH) / "data" / "raw"
    DRIVE_RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)
    print(f"Raw data destination on Drive: {DRIVE_RAW_DATA_DIR}")

    # List of datasets to download (slug, destination_subdir_name)
    datasets_to_download = [
        ("harshwardhanbhangale/kyoto-2006", "kyoto")
        # ,("solarmainframe/ids-intrusion-csv", "cicids2018"), # Corrected slug
        # ("harshwardhanbhangale/unsw-complete-dataset", "unsw")
    ]

    print(f"\nAttempting to download {len(datasets_to_download)} datasets to Google Drive...")

    # Loop through datasets with a tqdm progress bar
    for slug, subdir in tqdm(datasets_to_download, desc="Downloading Datasets"):
        print(f"\n--- Starting download for: {slug} ---")
        DEST_DRIVE = DRIVE_RAW_DATA_DIR / subdir
        DEST_DRIVE.mkdir(exist_ok=True)

        # Construct Kaggle download command
        # Use -p to specify the Drive path, --unzip to extract, --force to overwrite
        download_command = f"kaggle datasets download -d {slug} -p {str(DEST_DRIVE)} --unzip --force"

        print(f"Running command: !{download_command}")
        # Execute the download command (progress depends on kaggle CLI output)
        # Using os.system to capture output might be complex, ! is simpler here
        !{download_command}

        print(f"--- Download attempt for {slug} finished. Check output above for status. ---")
        # Optional: Verify contents after download if needed
        # print(f"Contents of {DEST_DRIVE}:")
        # !ls -lh {str(DEST_DRIVE)}


    print("\n✅ All dataset download attempts finished.")
    print("Files should be stored persistently in:", DRIVE_RAW_DATA_DIR)

# --- Cell: Preview Kyoto Data Inside ZIP Files on Drive ---
import pandas as pd
import pathlib
import glob
import os
import zipfile
import io # Required for reading from zip stream
from tqdm.notebook import tqdm
import itertools # Required for islice

# --- IMPORTANT: Define the path where your Kyoto ZIP file(s) are located ON YOUR DRIVE ---
# Adjust '/YourNetLingoProjectFolder/data/raw/kyoto_zips' to the correct path within 'My Drive'
DRIVE_RAW_KYOTO_ZIPS_PATH = "/content/drive/MyDrive/NetLingo_project/data/raw/"
# --- Make sure the path above is correct! ---

# Define the 24 columns for Kyoto files
KYOTO_COLS = [
    "duration","service","src_bytes","dst_bytes","count","same_srv_rate",
    "serror_rate","srv_serror_rate","dst_host_count","dst_host_srv_count",
    "dst_host_same_src_port_rate","dst_host_serror_rate",
    "dst_host_srv_serror_rate","flag",
    "ids_alert","malware_alert","ashula_alert","label",
    "src_ip","src_port","dst_ip","dst_port","start_time","protocol"
]

def display_df(df, name=""):
    """Helper to display DataFrame"""
    if df is not None and not df.empty:
        print(f"\n=== Top 10 rows from: {name} ===")
        display(df)
    else:
        print(f"\n--- Could not read data or empty file: {name} ---")

# Find all .zip files in the specified Drive directory
drive_kyoto_zip_dir = pathlib.Path(DRIVE_RAW_KYOTO_ZIPS_PATH)
zip_files = list(drive_kyoto_zip_dir.glob('*.zip'))

if not zip_files:
    print(f"❗️ No .zip files found in {DRIVE_RAW_KYOTO_ZIPS_PATH}. Please check the path.")
else:
    print(f"Found {len(zip_files)} zip file(s) to inspect.")

    # Process each zip file found
    for zip_path in tqdm(zip_files, desc="Processing ZIP Files"):
        print(f"\n--- Inspecting ZIP: {zip_path.name} ---")
        try:
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                # Get list of all files inside the zip
                all_files_in_zip = zip_ref.namelist()
                # Filter for likely data files (e.g., .txt, maybe inside directories)
                # Adjust '*.txt' if the data files have a different extension or path structure inside the zip
                data_files_in_zip = [f for f in all_files_in_zip if f.endswith('.txt') and not f.startswith('__MACOSX')]

                if not data_files_in_zip:
                    print(f"  No .txt data files found directly inside {zip_path.name}")
                    # Optional: Add logic here to look deeper into subdirectories if needed
                    continue

                print(f"  Found {len(data_files_in_zip)} potential data file(s) inside.")

                # Preview the first few data files found within this zip
                files_to_preview = data_files_in_zip[:5] # Limit preview to first 5 txt files per zip
                for member_name in tqdm(files_to_preview, desc=f"Previewing files in {zip_path.name}", leave=False):
                    df_preview = None
                    try:
                        # Open the member file as a stream without full extraction
                        with zip_ref.open(member_name) as member_file:
                            # Read first 10 lines (plus maybe a few extra for safety)
                            lines = []
                            # Use TextIOWrapper to handle potential encoding issues
                            # Try 'latin1' first as it often works for mixed data
                            try:
                                wrapper = io.TextIOWrapper(member_file, encoding='latin1')
                                for line in itertools.islice(wrapper, 12): # Read up to 12 lines
                                    lines.append(line)
                            except UnicodeDecodeError:
                                print(f"    Trying utf-8 for {member_name}...")
                                # Reset stream position if possible (might not be needed with islice)
                                member_file.seek(0)
                                wrapper = io.TextIOWrapper(member_file, encoding='utf-8')
                                lines = []
                                for line in itertools.islice(wrapper, 12):
                                    lines.append(line)

                            if not lines:
                                print(f"    Could not read lines from {member_name}")
                                continue

                            # Join the lines and read into pandas using StringIO
                            data_string = "".join(lines)
                            df_preview = pd.read_csv(
                                io.StringIO(data_string),
                                sep="\t",
                                header=None,
                                names=KYOTO_COLS,
                                engine="python",
                                on_bad_lines="warn", # Warn instead of skip for preview
                                nrows=10 # Ensure we only take 10 rows max
                            )
                    except Exception as e:
                        print(f"    ❗️ Error reading member {member_name} from {zip_path.name}: {e}")

                    # Display the DataFrame for this member file
                    display_df(df_preview, f"{zip_path.name} -> {member_name}")

        except zipfile.BadZipFile:
            print(f"❗️ Error: {zip_path.name} is not a valid zip file or is corrupted.")
        except Exception as e:
            print(f"❗️ An unexpected error occurred processing {zip_path.name}: {e}")

print("\n--- Finished inspecting all zip files ---")

# --- Cell: CIC-IDS 2018 Preview (from Drive) ---
import pandas as pd
import pathlib
import glob
import os
from tqdm.notebook import tqdm

# --- IMPORTANT: Define the path where your unzipped CIC-IDS CSV files are located ON YOUR DRIVE ---
# Adjust '/YourNetLingoProjectFolder/data/raw/cicids2018_on_drive' to the correct path within 'My Drive'
DRIVE_RAW_CIC_PATH = "/content/drive/MyDrive/NetLingo_project/data/raw/cicids2018"
# --- Make sure the path above is correct! ---

def show_first_n(df, n=50, name=""):
    """Helper function to display head of dataframe"""
    print(f"\n=== {name}  – Top {n} rows ===")
    if df is not None and not df.empty:
        display(df.head(n))
    else:
        print(f"--- DataFrame is empty or None for {name} ---")

# Find all .csv files in the specified Drive directory
drive_cic_dir = pathlib.Path(DRIVE_RAW_CIC_PATH)
cic_files = sorted(list(drive_cic_dir.glob('*.csv'))) # Find all CSVs

if not cic_files:
     print(f"❗️ No CIC-IDS .csv files found in {DRIVE_RAW_CIC_PATH}. Please check the path.")
else:
    print(f"Found {len(cic_files)} CIC-IDS files on Drive to preview.")

    # Preview the first few files (e.g., up to 5)
    for file_path in tqdm(cic_files[:5], desc="Previewing CIC-IDS Files"):
        df_preview = None
        try:
            # Read only the first N rows to preview, assume header is row 0
            df_preview = pd.read_csv(
                file_path,
                nrows=50,
                low_memory=False # To potentially avoid DtypeWarning on preview
            )
            # Clean potential whitespace from column names
            df_preview.columns = df_preview.columns.str.strip()
        except Exception as e:
            print(f"    ❗️ Error reading file {file_path.name}: {e}")

        # Display the DataFrame for this file
        show_first_n(df_preview, 50, file_path.name)

print("\n--- Finished previewing CIC-IDS files ---")

# --- Cell: Preview UNSW_NB15_training-set.csv (Fixing BOM) ---
import pandas as pd
import pathlib
import os

# --- Using the path you provided ---
DRIVE_RAW_UNSW_PATH = "/content/drive/MyDrive/NetLingo_project/data/raw/unsw"
TRAINING_SET_FILE = pathlib.Path(DRIVE_RAW_UNSW_PATH) / "UNSW_NB15_training-set.csv"
# ---

# Set float format to potentially silence display warnings
pd.set_option("display.float_format", "{:,.6g}".format)

def show_first_n(df, n=50, name=""):
    """Helper function to display head of dataframe"""
    print(f"\n=== {name}  – Top {n} rows ===")
    if df is not None and not df.empty:
        # Clean column names loaded from header
        df.columns = df.columns.str.strip()
        # Remove BOM manually IF utf-8-sig didn't catch it (extra safety)
        if df.columns[0].startswith('ï»¿'):
             print("--- Manually removing BOM from first column name ---")
             df.rename(columns={df.columns[0]: df.columns[0][3:]}, inplace=True)

        print(f"--- Columns loaded: {list(df.columns[:5])} ... (Total: {len(df.columns)})") # Show first few + count
        display(df.head(n))
    else:
         print(f"--- DataFrame is empty or None for {name} ---")

# Check if the specific training set file exists
if not TRAINING_SET_FILE.exists():
    print(f"❗️ ERROR: Cannot find file: {TRAINING_SET_FILE}")
    print(f"❗️ Please ensure 'UNSW_NB15_training-set.csv' exists at that exact path on your Drive.")
else:
    print(f"Found training set file: {TRAINING_SET_FILE}")
    print("Attempting to read first 50 rows using its header with utf-8-sig encoding...")
    df_preview = None
    try:
        df_preview = pd.read_csv(
            TRAINING_SET_FILE,
            header=0, # Explicitly use the first row as the header
            nrows=50, # Read only 50 rows for preview
            low_memory=False, # Avoid DtypeWarning
            # --- THE ONLY CHANGE: Use utf-8-sig to handle BOM ---
            encoding="utf-8-sig"
            # ---
        )
    except Exception as e:
         print(f"    ❗️ Error reading file {TRAINING_SET_FILE.name}: {e}")

    # Display the preview
    show_first_n(df_preview, 50, TRAINING_SET_FILE.name)

print("\n--- Finished previewing UNSW_NB15_training-set.csv ---")

# --- Cell: Kyoto 2006+ Tokenization (ROBUST: Member-Level State Tracking) ---
import pandas as pd
import pathlib
import glob
import os
import zipfile
import io
from tqdm.notebook import tqdm
import math
import numpy as np
import gc
import time # For slight delay after state write

# --- Configuration ---
# Path where your Kyoto ZIP file(s) are located ON YOUR DRIVE
DRIVE_RAW_KYOTO_ZIPS_PATH = "/content/drive/MyDrive/NetLingo_project/data/raw"
# Output path on Drive (Should be defined by previous cell)
# Example: OUT_KYOTO_DRIVE = pathlib.Path("/content/drive/MyDrive/NetLingo_project/data/tokens/kyoto_tokens.txt")
# State file to track completed *member* files
STATE_FILE_MEMBERS = pathlib.Path(DRIVE_RAW_KYOTO_ZIPS_PATH) / "kyoto_processed_members.txt"
# ---
# Define the base path for your project on Google Drive
DRIVE_BASE_PATH = '/content/drive/MyDrive/'
# --- CORRECTION: Remove leading slash ---
PROJECT_FOLDER_NAME = 'NetLingo_project' # CHANGE THIS if you used a different folder name
# ---
DRIVE_PROJECT_PATH = os.path.join(DRIVE_BASE_PATH, PROJECT_FOLDER_NAME)

# Define where the final token text files will be saved ON GOOGLE DRIVE
DRIVE_TOKENS_DIR = pathlib.Path(DRIVE_PROJECT_PATH) / "data" / "tokens"

# Create the directories if they don't exist within your Drive
DRIVE_TOKENS_DIR.mkdir(parents=True, exist_ok=True)

# Define the full paths for the output files
OUT_KYOTO_DRIVE = DRIVE_TOKENS_DIR / "kyoto_tokens.txt"
#OUT_CIC_DRIVE = DRIVE_TOKENS_DIR / "cic_tokens.txt"
#OUT_UNSW_DRIVE = DRIVE_TOKENS_DIR / "unsw_tokens.txt"

print(f"Base project path on Drive: {DRIVE_PROJECT_PATH}")
print(f"Token output directory on Drive: {DRIVE_TOKENS_DIR}")
#print(f"Kyoto output file path: {OUT_KYOTO_DRIVE}")
print(f"Starting Kyoto tokenization (ROBUST - Member-Level State)...")
if 'OUT_KYOTO_DRIVE' not in locals():
     raise NameError("OUT_KYOTO_DRIVE is not defined. Run the path setup cell.")
else:
    print(f"Output will be saved to: {OUT_KYOTO_DRIVE}")
    print(f"State file: {STATE_FILE_MEMBERS}")

# --- Define columns, maps, bucketing (same as before) ---
KYOTO_COLS = [
    "duration","service","src_bytes","dst_bytes","count","same_srv_rate", "serror_rate","srv_serror_rate",
    "dst_host_count","dst_host_srv_count", "dst_host_same_src_port_rate","dst_host_serror_rate",
    "dst_host_srv_serror_rate","flag", "ids_alert","malware_alert","ashula_alert","label",
    "src_ip","src_port","dst_ip","dst_port","start_time","protocol"
]
KYOTO_MAP = {
    "duration": "DUR_",     "service": "SERVICE_", "src_bytes":"SBYTES_",  "dst_bytes":"DBYTES_",
    "count":    "PKTS_",    "flag":     "FLAGS_",   "src_ip":   "SRCIP_",   "dst_ip":   "DSTIP_",
    "src_port": "SPORT_",   "dst_port": "DPORT_",   "protocol": "PROTO_"
}
NUMERIC_COLS_KYOTO = ["duration", "src_bytes", "dst_bytes", "count"]
BUCKETS = {
    "duration":  [(0, 1, "0-1s"), (1, 10, "1-10s"), (10, 60, "10-60s"), (60, np.inf, ">60s")],
    "src_bytes": [(0, 100, "0-100B"), (100, 1000, "100B-1K"), (1000, 10000, "1K-10K"), (10000, np.inf, ">10K")],
    "dst_bytes": [(0, 100, "0-100B"), (100, 1000, "100B-1K"), (1000, 10000, "1K-10K"), (10000, np.inf, ">10K")],
    "count":     [(0, 5, "0-5"), (5, 50, "5-50"), (50, 200, "50-200"), (200, np.inf, ">200")],
}
# --- Bucket and Sentence Functions (same as before) ---
def bucket_value(val, col_name):
    if col_name not in BUCKETS: return str(val).strip().replace(" ", "_").upper()
    try:
        num_val = float(val)
        if not np.isfinite(num_val): return f"NonFinite_{col_name}"
        for low, high, label in BUCKETS[col_name]:
            if low <= num_val < high: return label
        if BUCKETS[col_name] and num_val >= BUCKETS[col_name][-1][0]: return BUCKETS[col_name][-1][2]
        return f"OutOfRange_{col_name}"
    except (ValueError, TypeError): return f"NaN_{col_name}"

def flow_to_sentence_kyoto(row, cmap, tag="DS_KYOTO"):
    toks = [tag]
    for col, pref in cmap.items():
        if col in row and pd.notna(row[col]):
            if col in NUMERIC_COLS_KYOTO: val_str = bucket_value(row[col], col)
            else: val_str = str(row[col]).strip().replace(" ", "_").upper()
            if val_str and not val_str.startswith("NaN_") and not val_str.startswith("NonFinite_"):
                 toks.append(f"{pref}{val_str}")
    label_val = row.get("label", None)
    verdict = "ATTACK" if label_val in [-1, -2] else "BENIGN"
    toks.append(f"LABEL_{verdict}")
    return " ".join(toks)
# --- End Function Definitions ---

# --- Step 1: Get Master List of all .txt members in all target zips ---
master_member_list = []
drive_kyoto_zip_dir = pathlib.Path(DRIVE_RAW_KYOTO_ZIPS_PATH)
zip_files = sorted(list(drive_kyoto_zip_dir.glob('20*.zip')) + list(drive_kyoto_zip_dir.glob('kyoto*.zip')))
zip_files = sorted(list(set(zip_files)))
print(f"Scanning {len(zip_files)} zip files to build master list of members...")
for zip_path in tqdm(zip_files, desc="Scanning Zips"):
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            members = [m for m in zip_ref.namelist() if m.endswith('.txt') and not m.startswith('__MACOSX')]
            # Store as "zip_filename/member_filename"
            master_member_list.extend([f"{zip_path.name}/{member}" for member in members])
    except zipfile.BadZipFile:
        print(f"Warning: Skipping bad zip file {zip_path.name}")
    except Exception as e:
        print(f"Warning: Error scanning zip {zip_path.name}: {e}")

master_member_list = sorted(master_member_list)
total_members_overall = len(master_member_list)
print(f"Found {total_members_overall} total .txt member files across all zip archives.")

# --- Step 2: Load the set of already processed members ---
processed_members = set()
if STATE_FILE_MEMBERS.exists():
    try:
        with open(STATE_FILE_MEMBERS, 'r') as f_state:
            processed_members = set(line.strip() for line in f_state if line.strip())
        print(f"Read {len(processed_members)} processed members from state file.")
        # Determine file open mode based on state file presence
        MODE = "a" if processed_members else "w"
    except Exception as e:
        print(f"Warning: Could not read state file {STATE_FILE_MEMBERS}: {e}. Assuming fresh start.")
        MODE = "w"
else:
    MODE = "w"
    print("No state file found. Starting fresh run.")

# --- Step 3: Determine member files that need processing ---
pending_members = [m for m in master_member_list if m not in processed_members]
total_pending = len(pending_members)
print(f"Found {total_pending} member files remaining to process.")

# --- Step 4: Process Pending Files ---
if total_pending == 0:
    print("✅ All files seem to be processed already!")
else:
    lines_written_this_run = 0
    # Open output file in append mode ('a') or write ('w') if starting fresh
    # Open state file in append mode ('a')
    try:
        with open(OUT_KYOTO_DRIVE, MODE, encoding='utf-8') as fout, \
             open(STATE_FILE_MEMBERS, 'a', encoding='utf-8') as f_state:

            print(f"Opened output file {OUT_KYOTO_DRIVE} in mode '{MODE}'")
            print(f"Opened state file {STATE_FILE_MEMBERS} in mode 'a'")

            # Loop through only the PENDING member files
            for member_identifier in tqdm(pending_members, desc="Processing Pending TXT Files"):
                try:
                    zip_filename, member_name = member_identifier.split('/', 1)
                    zip_path = drive_kyoto_zip_dir / zip_filename

                    # Open the required zip file
                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                        # Open the specific member stream
                        with zip_ref.open(member_name) as member_file:
                            wrapper = io.TextIOWrapper(member_file, encoding='latin1')
                            df = pd.read_csv(
                                wrapper, sep="\t", header=None, names=KYOTO_COLS,
                                engine="python", on_bad_lines="warn", dtype=str
                            )

                        # Process the dataframe (convert types, drop bad rows, tokenize)
                        numeric_cols_to_convert = NUMERIC_COLS_KYOTO + ['label']
                        for ncol in numeric_cols_to_convert:
                            if ncol in df.columns: df[ncol] = pd.to_numeric(df[ncol], errors='coerce')
                        df = df.dropna(subset=['label'])
                        if df.empty:
                            print(f"  Skipping empty or invalid member: {member_identifier}")
                            # Still mark as processed if it was read successfully but empty/invalid
                            f_state.write(f"{member_identifier}\n")
                            processed_members.add(member_identifier)
                            continue
                        df['label'] = df['label'].astype(int)

                        # Write sentences to output file
                        member_lines = 0
                        for _, row in df.iterrows():
                            sentence = flow_to_sentence_kyoto(row, KYOTO_MAP)
                            fout.write(sentence + "\n")
                            member_lines += 1
                        lines_written_this_run += member_lines

                    # --- CRUCIAL: Update state file AFTER member is fully processed ---
                    f_state.write(f"{member_identifier}\n")
                    f_state.flush() # Ensure it's written to disk immediately
                    processed_members.add(member_identifier) # Update in-memory set
                    # time.sleep(0.01) # Small optional delay

                    # Optional: print progress periodically
                    # if len(processed_members) % 100 == 0:
                    #    print(f"  Processed {len(processed_members)} / {total_members_overall} members...")

                except pd.errors.EmptyDataError:
                    print(f"  Skipping completely empty member file: {member_identifier}")
                    f_state.write(f"{member_identifier}\n"); f_state.flush(); processed_members.add(member_identifier)
                except FileNotFoundError:
                     print(f"❗️ Error: Zip file not found for member {member_identifier}. Skipping.")
                     # Don't add to processed if zip is missing
                except KeyError as e_key:
                     print(f"❗️ KeyError processing member {member_identifier} (likely bad column name: {e_key}). Skipping.")
                     f_state.write(f"{member_identifier}\n"); f_state.flush(); processed_members.add(member_identifier)
                except Exception as e_member:
                    print(f"❗️ Unhandled error processing member {member_identifier}: {e_member}. Skipping.")
                    # Decide whether to mark skipped files as processed or retry next time
                    # Marking as processed to avoid getting stuck:
                    f_state.write(f"{member_identifier}\n"); f_state.flush(); processed_members.add(member_identifier)

            # --- End Pending Member Loop ---
        print(f"\n✅ Finished current run. Processed {lines_written_this_run} new lines.")
        print(f"Total processed members recorded in state file: {len(processed_members)}")

    except Exception as e_outer:
        print(f"\n❗️ CRITICAL ERROR during file processing: {e_outer}")
        print("❗️ State file might not reflect partial progress from this run.")

    print("Verifying output file on Drive:")
    !ls -lh {str(OUT_KYOTO_DRIVE)}

# --- Cell: CIC-IDS 2018 Tokenization (Read CSVs from Drive, Save Tokens to Drive) ---
import pandas as pd
import pathlib
import glob
import os
from tqdm.notebook import tqdm
import numpy as np
import gc

# Define the base path for your project on Google Drive
DRIVE_BASE_PATH = '/content/drive/MyDrive/'
# --- CORRECTION: Remove leading slash ---
PROJECT_FOLDER_NAME = 'NetLingo_project' # CHANGE THIS if you used a different folder name
# ---
DRIVE_PROJECT_PATH = os.path.join(DRIVE_BASE_PATH, PROJECT_FOLDER_NAME)

# Define where the final token text files will be saved ON GOOGLE DRIVE
DRIVE_TOKENS_DIR = pathlib.Path(DRIVE_PROJECT_PATH) / "data" / "tokens"

# Create the directories if they don't exist within your Drive
DRIVE_TOKENS_DIR.mkdir(parents=True, exist_ok=True)

# Define the full paths for the output files
#OUT_KYOTO_DRIVE = DRIVE_TOKENS_DIR / "kyoto_tokens.txt"
OUT_CIC_DRIVE = DRIVE_TOKENS_DIR / "cic_tokens.txt"
#OUT_UNSW_DRIVE = DRIVE_TOKENS_DIR / "unsw_tokens.txt"

print(f"Base project path on Drive: {DRIVE_PROJECT_PATH}")
print(f"Token output directory on Drive: {DRIVE_TOKENS_DIR}")
#print(f"Kyoto output file path: {OUT_KYOTO_DRIVE}")
print(f"CIC-IDS output file path: {OUT_CIC_DRIVE}")
#print(f"UNSW output file path: {OUT_UNSW_DRIVE}")

# --- Using the path you provided ---
DRIVE_RAW_CIC_PATH = "/content/drive/MyDrive/NetLingo_project/data/raw/cicids2018"
# --- Output path on Drive (Should be defined already) ---
# OUT_CIC_DRIVE = DRIVE_TOKENS_DIR / "cic_tokens.txt"
# ---
# --- Processing Parameters ---
CHUNK_SIZE = 100000  # Process 100,000 rows at a time (adjust based on RAM/speed)
MODE = "w" # Start with "w" (write) for a clean file. Change to "a" (append) ONLY if you are SURE a previous run fully completed some files.
# ---

print(f"Starting CIC-IDS 2018 tokenization (Reading from: {DRIVE_RAW_CIC_PATH})...")
print(f"Output will be saved to: {OUT_CIC_DRIVE}")
print(f"Processing in chunks of {CHUNK_SIZE} rows.")

# Define column map for CIC-IDS 2018 (Add more columns as needed)
CIC_MAP = {
    'Src IP':       "SRCIP_",    'Dst IP':       "DSTIP_",
    'Src Port':     "SPORT_",    'Dst Port':     "DPORT_",
    'Protocol':     "PROTO_",    'Flow Duration':"DUR_",
    'Tot Fwd Pkts': "PKTS_",     'Tot Bwd Pkts': "BWD_PKTS_",
    'TotLen Fwd Pkts':"BYTES_",   'TotLen Bwd Pkts':"BWD_BYTES_",
    'Flow Byts/s':  "FLOW_BPS_", 'Flow Pkts/s':  "FLOW_PPS_"
}

# Define numeric columns and bucketing for CIC-IDS (adjust ranges/columns)
NUMERIC_COLS_CIC = ['Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Flow Byts/s', 'Flow Pkts/s']
BUCKETS_CIC = {
    'Flow Duration': [(0, 1e3, "0-1ms"), (1e3, 1e6, "1ms-1s"), (1e6, 60e6, "1s-1m"), (60e6, np.inf, ">1m")],
    'Tot Fwd Pkts':  [(0, 5, "0-5"), (5, 50, "5-50"), (50, 200, "50-200"), (200, np.inf, ">200")],
    'Tot Bwd Pkts':  [(0, 5, "0-5"), (5, 50, "5-50"), (50, 200, "50-200"), (200, np.inf, ">200")],
    'TotLen Fwd Pkts':[(0, 100, "0-100B"), (100, 1000, "100B-1K"), (1000, 10000, "1K-10K"), (10000, np.inf, ">10K")],
    'TotLen Bwd Pkts':[(0, 100, "0-100B"), (100, 1000, "100B-1K"), (1000, 10000, "1K-10K"), (10000, np.inf, ">10K")],
    'Flow Byts/s':  [(0, 1e3, "0-1KBps"), (1e3, 1e6, "1KBps-1MBps"), (1e6, np.inf, ">1MBps")],
    'Flow Pkts/s':  [(0, 100, "0-100pps"), (100, 10000, "100pps-10Kpps"), (10000, np.inf, ">10Kpps")],
}

def bucket_value_cic(val, col_name):
    """Applies bucketing rules for CIC-IDS."""
    val_str = str(val).strip()
    if 'infinity' in val_str.lower(): return "INF"
    if col_name not in BUCKETS_CIC: return val_str.replace(" ", "_")
    try:
        num_val = float(val_str)
        if not np.isfinite(num_val): return f"NonFinite_{col_name}"
        for low, high, label in BUCKETS_CIC[col_name]:
            if low <= num_val < high: return label
        if BUCKETS_CIC[col_name] and num_val >= BUCKETS_CIC[col_name][-1][0]:
            return BUCKETS_CIC[col_name][-1][2]
        return f"OutOfRange_{col_name}"
    except (ValueError, TypeError): return f"NaN_{col_name}"

def flow_to_sentence_cic(row, cmap, label_col_name, tag="DS_CIC"):
    """Converts a CIC-IDS DataFrame row to a token sentence with bucketing."""
    toks = [tag]
    for col, pref in cmap.items():
         if col in row and pd.notna(row[col]):
             if col in NUMERIC_COLS_CIC: val_str = bucket_value_cic(row[col], col)
             else: val_str = str(row[col]).strip().replace(" ", "_")
             if val_str and not val_str.startswith("NaN_") and not val_str.startswith("NonFinite_"):
                toks.append(f"{pref}{val_str}")
    verdict = "ATTACK" if row[label_col_name] != "Benign" else "BENIGN"
    toks.append(f"LABEL_{verdict}")
    return " ".join(toks)

# Find all CIC-IDS .csv files FROM THE SPECIFIED GOOGLE DRIVE PATH
drive_cic_dir = pathlib.Path(DRIVE_RAW_CIC_PATH)
cic_files = sorted(list(drive_cic_dir.glob('*.csv')))

if not cic_files:
     print(f"❗️ No CIC-IDS .csv files found in {DRIVE_RAW_CIC_PATH}. Please check the path.")
else:
    print(f"Found {len(cic_files)} CIC-IDS files on Drive to process.")
    lines_written_total = 0
    # Open output file ON GOOGLE DRIVE
    with open(OUT_CIC_DRIVE, MODE, encoding='utf-8') as fout:
        print(f"Opened {OUT_CIC_DRIVE} in mode '{MODE}'")
        # Outer loop: Iterate through each CSV file
        for file_path in tqdm(cic_files, desc="Processing CIC-IDS Files"):
            print(f"\n--- Processing file: {file_path.name} ---")
            try:
                # Read header once to get column names and find Label column
                header_df = pd.read_csv(file_path, nrows=0, low_memory=False)
                header_df.columns = header_df.columns.str.strip()
                label_col_name = next((col for col in header_df.columns if 'label' in col.lower()), 'Label')
                if label_col_name not in header_df.columns:
                     print(f"❗️ Label column '{label_col_name}' not found in {file_path.name}. Skipping file.")
                     continue

                # Use pd.read_csv with chunksize to iterate through the file
                chunk_iter = pd.read_csv(
                    file_path,
                    low_memory=False,
                    chunksize=CHUNK_SIZE # Process file in chunks
                )

                lines_in_file = 0
                # Inner loop: Process each chunk
                for chunk_df in tqdm(chunk_iter, desc=f"Chunks in {file_path.name}", leave=False):
                    # Clean column names for the chunk
                    chunk_df.columns = chunk_df.columns.str.strip()
                    # Process rows within the current chunk
                    for _, row in chunk_df.iterrows(): # Iterating over chunk is fast
                        sentence = flow_to_sentence_cic(row, CIC_MAP, label_col_name)
                        fout.write(sentence + "\n")
                        lines_in_file += 1

                lines_written_total += lines_in_file
                print(f"--- Finished file: {file_path.name}. Wrote {lines_in_file} lines. ---")
                gc.collect() # Suggest garbage collection after processing a large file

            except Exception as e:
                 print(f"❗️ Error processing {file_path.name}: {e}")

    # Final count summary
    if MODE == 'a':
        print(f"\n✅ Finished CIC-IDS. Appended approx {lines_written_total} lines this run to {OUT_CIC_DRIVE}")
    else:
        print(f"\n✅ Finished CIC-IDS. Wrote {lines_written_total} lines to {OUT_CIC_DRIVE}")
        print("Verifying file on Drive:")
        !ls -lh {str(OUT_CIC_DRIVE)}

# --- Cell: UNSW-NB15 Tokenization (Debug Label Column Issue) ---
import pandas as pd
import pathlib
import os
from tqdm.notebook import tqdm
import numpy as np
import gc

# --- Using the path you provided ---
DRIVE_RAW_UNSW_PATH = "/content/drive/MyDrive/NetLingo_project/data/raw/unsw"
TRAINING_SET_FILE = pathlib.Path(DRIVE_RAW_UNSW_PATH) / "UNSW_NB15_training-set.csv"
# ---

# --- Output path on Drive (Should be defined already) ---
# OUT_UNSW_DRIVE = DRIVE_TOKENS_DIR / "unsw_tokens.txt"
# ---

# --- Processing Parameters ---
CHUNK_SIZE = 100000
MODE = "w"
# ---

print(f"Starting UNSW-NB15 tokenization (Reading: {TRAINING_SET_FILE})...")
print(f"Output will be saved to: {OUT_UNSW_DRIVE}")
print(f"Processing in chunks of {CHUNK_SIZE} rows.")

# Define column map for UNSW-NB15 (Review/Adjust based on successful preview)
UNSW_MAP = {
    'dur':     'DUR_',      'proto':   'PROTO_',
    'service': 'SERVICE_',  'state':   'STATE_',
    'spkts':   'SPKTS_',    'dpkts':   'DPKTS_',
    'sbytes':  'SBYTES_',   'dbytes':  'DBYTES_',
    'rate':    'RATE_',     'sttl':    'STTL_',
    'dttl':    'DTTL_',     'sload':   'SLOAD_',
    'dload':   'DLOAD_',    'sloss':   'SLOSS_',
    'dloss':   'DLOSS_',    'sinpkt':  'SINPKT_',
    'dinpkt':  'DINPKT_',   'sjit':    'SJIT_',
    'djit':    'DJIT_',     'swin':    'SWIN_',
    'stcpb':   'STCPB_',    'dtcpb':   'DTCPB_',
    'dwin':    'DWIN_',     'tcprtt':  'TCPRTT_',
    'synack':  'SYNACK_',   'ackdat':  'ACKDAT_',
    'smeansz': 'SMEANSZ_',  'dmeansz': 'DMEANSZ_',
    'trans_depth': 'TRANS_DEPTH_', 'res_bdy_len': 'RES_BDY_LEN_',
    'ct_srv_src':  'CT_SRV_SRC_',  'ct_state_ttl': 'CT_STATE_TTL_',
    'ct_dst_ltm':   'CT_DST_LTM_', 'ct_src_dport_ltm': 'CT_SRC_DPORT_LTM_',
    'ct_dst_sport_ltm': 'CT_DST_SPORT_LTM_', 'ct_dst_src_ltm':   'CT_DST_SRC_LTM_',
    'is_ftp_login': 'IS_FTP_LOGIN_', 'ct_ftp_cmd':   'CT_FTP_CMD_',
    'ct_flw_http_mthd': 'CT_FLW_HTTP_MTHD_', 'ct_src_ltm':   'CT_SRC_LTM_',
    'ct_srv_dst':   'CT_SRV_DST_', 'is_sm_ips_ports': 'IS_SM_IPS_PORTS_'
}

# Define numeric columns for bucketing in UNSW
NUMERIC_COLS_UNSW = [
    'dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl',
    'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit',
    'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat',
    'smeansz', 'dmeansz', 'res_bdy_len'
]
# Define BUCKETS_UNSW
BUCKETS_UNSW = {
    'dur':       [(0, 0.1, "0-0.1s"), (0.1, 1, "0.1-1s"), (1, 10, "1-10s"), (10, np.inf, ">10s")],
    'sbytes':    [(0, 100, "0-100B"), (100, 1000, "100B-1K"), (1000, 100000, "1K-100K"), (100000, np.inf, ">100K")],
    # ... Add other bucket definitions as before ...
    'dmeansz':   [(0, 50, "0-50B"), (50, 200, "50-200B"), (200, 1000, "200B-1K"), (1000, np.inf, ">1K")],
}

def bucket_value_unsw(val, col_name):
    """Applies bucketing rules for UNSW."""
    if col_name not in BUCKETS_UNSW: return str(val).strip().replace(" ", "_")
    try:
        num_val = float(val)
        if not np.isfinite(num_val): return f"NonFinite_{col_name}"
        for low, high, label in BUCKETS_UNSW[col_name]:
            if low <= num_val < high: return label
        if BUCKETS_UNSW[col_name] and num_val >= BUCKETS_UNSW[col_name][-1][0]:
             return BUCKETS_UNSW[col_name][-1][2]
        return f"OutOfRange_{col_name}"
    except (ValueError, TypeError): return f"NaN_{col_name}"

# Note: label_col parameter removed, determined inside
def flow_to_sentence_unsw(row, cmap, actual_label_col_name, tag="DS_UNSW"):
    """Converts a UNSW DataFrame row to a token sentence with bucketing."""
    toks = [tag]
    for col, pref in cmap.items():
         if col in row and pd.notna(row[col]):
             if col in NUMERIC_COLS_UNSW: val_str = bucket_value_unsw(row[col], col)
             else: val_str = str(row[col]).strip().replace(" ", "_").upper()
             if val_str and not val_str.startswith("NaN_") and not val_str.startswith("NonFinite_"):
                 toks.append(f"{pref}{val_str}")
    # Use the actual label column name passed to the function
    verdict = "ATTACK" if row[actual_label_col_name] == 1 else "BENIGN"
    toks.append(f"LABEL_{verdict}")
    return " ".join(toks)

# Check if the specific training set file exists
if not TRAINING_SET_FILE.exists():
    print(f"❗️ ERROR: Cannot find file: {TRAINING_SET_FILE}")
else:
    lines_written_total = 0
    # Open output file ON GOOGLE DRIVE
    with open(OUT_UNSW_DRIVE, MODE, encoding='utf-8') as fout:
        print(f"Opened {OUT_UNSW_DRIVE} in mode '{MODE}'")
        try:
            print(f"Reading {TRAINING_SET_FILE.name} in chunks...")
            chunk_iter = pd.read_csv(
                TRAINING_SET_FILE,
                low_memory=False,
                encoding="utf-8-sig",
                chunksize=CHUNK_SIZE
            )

            first_chunk = True
            for chunk_df in tqdm(chunk_iter, desc=f"Processing Chunks"):
                # Clean column names for the chunk
                chunk_df.columns = chunk_df.columns.str.strip()

                # --- DEBUG: Print columns found in this chunk ---
                if first_chunk:
                    print(f"--- DEBUG: Columns in first chunk: {list(chunk_df.columns)}")
                    first_chunk = False
                # ---

                # --- Robustly find the label column (case-insensitive) ---
                actual_label_col_name = None
                for col in chunk_df.columns:
                    if 'label' in col.lower(): # Find column containing 'label' case-insensitively
                        actual_label_col_name = col
                        break # Found it
                # ---

                if actual_label_col_name is None:
                    print(f"❗️ ERROR: Could not find any column containing 'label' in this chunk's columns: {list(chunk_df.columns)}. Skipping chunk.")
                    continue # Skip this chunk if label col not found

                # Ensure label column is numeric
                chunk_df[actual_label_col_name] = pd.to_numeric(chunk_df[actual_label_col_name], errors='coerce')
                chunk_df = chunk_df.dropna(subset=[actual_label_col_name])
                if chunk_df.empty: continue # Skip if all rows had invalid labels
                chunk_df[actual_label_col_name] = chunk_df[actual_label_col_name].astype(int)

                # Process rows within the current chunk
                for _, row in chunk_df.iterrows():
                    # Pass the FOUND label column name to the function
                    sentence = flow_to_sentence_unsw(row, UNSW_MAP, actual_label_col_name)
                    fout.write(sentence + "\n")
                    lines_written_total += 1
                gc.collect()

        except Exception as e:
             print(f"❗️ Error processing {TRAINING_SET_FILE.name}: {e}")

    if MODE == 'a':
        print(f"\n✅ Finished UNSW. Appended approx {lines_written_total} lines this run to {OUT_UNSW_DRIVE}")
    else:
        print(f"\n✅ Finished UNSW. Wrote {lines_written_total} lines to {OUT_UNSW_DRIVE}")
        print("Verifying file on Drive:")
        !ls -lh {str(OUT_UNSW_DRIVE)}

# --- Cell: Combine Training Token Files ---
import os
import pathlib

# --- Ensure Drive Paths are Defined ---
# This relies on DRIVE_TOKENS_DIR being set correctly by your path definition cell, e.g.:
DRIVE_TOKENS_DIR = pathlib.Path("/content/drive/MyDrive/NetLingo_project/data/tokens")
if 'DRIVE_TOKENS_DIR' not in locals() or not os.path.exists(DRIVE_TOKENS_DIR):
     raise NameError("DRIVE_TOKENS_DIR is not defined or does not exist. Please run the path setup cell.")

# Define paths to the individual token files on Drive
KYOTO_TOKENS = DRIVE_TOKENS_DIR / "kyoto_tokens.txt"
UNSW_TOKENS = DRIVE_TOKENS_DIR / "unsw_tokens.txt"
# --- Assumption: cic_tokens.txt contains the processed data for the training split (e.g., Days 1-5) ---
# If you created a differently named file for the CIC training split, change the filename below
CIC_TOKENS_TRAIN = DRIVE_TOKENS_DIR / "cic_tokens.txt"
# ---

# Define the path for the combined output file on Drive
COMBINED_TRAIN_CORPUS = DRIVE_TOKENS_DIR / "train_corpus.txt"

# --- Check if input files exist before attempting concatenation ---
input_files_exist = True
files_to_check = [KYOTO_TOKENS, UNSW_TOKENS, CIC_TOKENS_TRAIN]
missing_files = []
for f_path in files_to_check:
    if not f_path.exists():
        print(f"❗️ ERROR: Input token file not found: {f_path}")
        missing_files.append(str(f_path))
        input_files_exist = False

if not input_files_exist:
    print("\n❗️ Cannot proceed with concatenation because one or more input files are missing.")
    # Optional: List contents of token dir to help debug
    # print(f"\nContents of {DRIVE_TOKENS_DIR}:")
    # !ls -lh {str(DRIVE_TOKENS_DIR)}
else:
    print("All input token files found. Combining...")
    # Use shell 'cat' command to combine files directly on Drive
    # Ensure correct paths and filenames are used
    !cat "{str(KYOTO_TOKENS)}" "{str(UNSW_TOKENS)}" "{str(CIC_TOKENS_TRAIN)}" > "{str(COMBINED_TRAIN_CORPUS)}"

    print(f"\nCombined training corpus created at: {COMBINED_TRAIN_CORPUS}")
    print("Verifying combined file size and line count (line count can take time):")
    !ls -lh "{str(COMBINED_TRAIN_CORPUS)}"
    !wc -l "{str(COMBINED_TRAIN_CORPUS)}"