# -*- coding: utf-8 -*-
"""Notebook 3: Inference & RAG Explanation (Colab - UGR Test)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WU1_54u87UIIQhcqFBggkpZuWBCDxUo3
"""

# ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
# ‚îÇ 0)  Disable vision back‚Äëends & provide dummy timm symbols    ‚îÇ
# ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
import sys, types, os, importlib.machinery as _mach

# 1) Tell ü§ó¬†Transformers NOT to try timm / torchvision at all
os.environ["TRANSFORMERS_NO_TIMM"]        = "1"
os.environ["TRANSFORMERS_NO_TORCHVISION"] = "1"

# 2) Build a minimal stub that satisfies the exact attributes asked for
timm_stub           = types.ModuleType("timm")
timm_data_stub      = types.ModuleType("timm.data")

class _Dummy:               # fake class used once
    pass

def _noop(*a, **k):         # fake function used once
    return None

# Provide the two symbols transformers looks for
timm_data_stub.ImageNetInfo          = _Dummy
timm_data_stub.infer_imagenet_subset = _noop

# wire the sub‚Äëmodule
timm_stub.data = timm_data_stub

# give importlib a dummy spec so it is treated as a ‚Äúreal‚Äù module
timm_stub.__spec__      = _mach.ModuleSpec("timm", loader=None)
timm_data_stub.__spec__ = _mach.ModuleSpec("timm.data", loader=None)

# register in sys.modules **before anything else is imported**
sys.modules["timm"]       = timm_stub
sys.modules["timm.data"]  = timm_data_stub

# optional: stub torchvision completely as well (saves further surprises)
torchvision_stub           = types.ModuleType("torchvision")
torchvision_stub.__spec__  = _mach.ModuleSpec("torchvision", loader=None)
sys.modules["torchvision"] = torchvision_stub

print("‚úÖ timm / torchvision stubs registered ‚Äì safe to import transformers now")

# ==============================================================================
# Cell 1: Installations
# ==============================================================================
print("--- Cell 1: Installing Libraries ---")
# Install libraries needed for inference, FAISS, embeddings, and Google Gemini
!pip uninstall -y torch torchvision torchaudio transformers accelerate bitsandbytes peft trl datasets numpy scipy sentencepiece tiktoken einops faiss-cpu sentence-transformers openai kaggle pandas google-generativeai
!pip install -q -U \
    "torch>=2.1" \
    "transformers>=4.40.0" \
    "datasets>=2.16.0" \
    "accelerate>=0.28.0" \
    "peft>=0.10.0" \
    "bitsandbytes>=0.43.2" \
    "trl>=0.8.6" \
    "faiss-cpu" \
    "sentence-transformers>=2.6.0" \
    "google-generativeai>=0.4.0" \
    "kaggle" \
    "pandas" \
    "numpy<2.1" \
    "scipy" \
    "sentencepiece" \
    "tiktoken" \
    "einops"

print("\nLibraries installed/updated.")

# ==============================================================================
# Cell 2: Imports
# ==============================================================================
print("\n--- Cell 2: Importing Libraries ---")
import torch
import os
import pandas as pd
import numpy as np
import pathlib
import json
import faiss
import re
import glob
from datasets import load_dataset, Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    pipeline,
    logging,
)
from peft import PeftModel
from sentence_transformers import SentenceTransformer
from tqdm.notebook import tqdm
import gc
import getpass # For securely entering API key
import google.generativeai as genai # Import Google Gemini library
# Removed tarfile, shutil as we load directly from Kaggle dataset CSVs
import transformers
import datasets as ds_datasets
import accelerate as acc_accelerate
import peft as peft_peft
import bitsandbytes as bnb

# Print versions
# ... (version printing code as before) ...
try: import google.generativeai as genai_check; print(f"Google GenAI Version: {genai_check.__version__}")
except ImportError: print("Google GenAI not imported.")

print("\nLibraries imported.")
# --- Verify GPU availability ---
if torch.cuda.is_available(): print(f"GPU detected: {torch.cuda.get_device_name(0)}"); torch.set_default_device('cuda')
else: print("‚ö†Ô∏è Warning: No GPU detected."); torch.set_default_device('cpu')

# ==============================================================================
# Cell‚ÄØ3 ¬∑ Mount Drive (Colab) & define project paths        ‚òÖ UPDATED ‚òÖ
# ==============================================================================

print("\n--- Cell‚ÄØ3: Mount Drive & define paths ---")

import os, pathlib, json, sys
from pprint import pprint

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 1)  Google¬†Drive  (only when running in Colab)                     ‚îÇ
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
try:
    from google.colab import drive        # will fail on Kaggle
    if not os.path.exists("/content/drive/MyDrive"):
        print("‚Ä¢ Mounting Google¬†Drive ‚Ä¶")
        drive.mount("/content/drive", force_remount=True)
    else:
        print("‚Ä¢ Google¬†Drive already mounted.")
    DRIVE_BASE = pathlib.Path("/content/drive/MyDrive")
except ImportError:
    # running on Kaggle or local Jupyter
    DRIVE_BASE = pathlib.Path.home() / "drive_mock"   # dummy placeholder
    DRIVE_BASE.mkdir(parents=True, exist_ok=True)
    print("‚Ä¢ Colab not detected ‚Äì using local drive path:", DRIVE_BASE)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 2)  Project root on Drive                                          ‚îÇ
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
PROJECT_NAME        = "NetLingo_project"
DRIVE_PROJ_PATH     = DRIVE_BASE / PROJECT_NAME
DRIVE_PROJ_PATH.mkdir(parents=True, exist_ok=True)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 3)  Fine‚Äëtuned Falcon adapters (already trained & saved on Drive)  ‚îÇ
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ADAPTER_REL_PATH    = "models/netlingo_falcon_lora_adapters"
ADAPTER_PATH_DRIVE  = DRIVE_PROJ_PATH / ADAPTER_REL_PATH

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 4)  FAISS data will be downloaded to a *RAM* folder in Colab       ‚îÇ
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
LOCAL_FAISS_DIR     = pathlib.Path("/content/faiss_data")
LOCAL_FAISS_DIR.mkdir(parents=True, exist_ok=True)
LOCAL_FAISS_INDEX_PATH = LOCAL_FAISS_DIR / "faiss_mitre_cve_index.bin"
LOCAL_ID_MAP_PATH      = LOCAL_FAISS_DIR / "faiss_mitre_cve_id_map.json"

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 5)  UGR16¬†test set download location  (‚òÖ new ‚Äì writable!)          ‚îÇ
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
LOCAL_UGR_DIR       = pathlib.Path("/content/ugr16_test")      # stays in RAM
# ‚ñ∫ If you prefer to persist on Drive, comment the line above and uncomment:
# LOCAL_UGR_DIR    = DRIVE_PROJ_PATH / "ugr16_test"
LOCAL_UGR_DIR.mkdir(parents=True, exist_ok=True)

# file names for the v2 variant ‚Äì change if you use v1 / v3 / ‚Ä¶
UGR_VERSION_PREFIX  = "UGR16v2"
UGR_XTEST_FILE      = f"{UGR_VERSION_PREFIX}_Xtest.csv"
UGR_YTEST_FILE      = f"{UGR_VERSION_PREFIX}_Ytest.csv"
UGR_XTEST_PATH      = LOCAL_UGR_DIR / UGR_XTEST_FILE
UGR_YTEST_PATH      = LOCAL_UGR_DIR / UGR_YTEST_FILE

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 6)  Kaggle dataset slugs (used later in Cell¬†4)                    ‚îÇ
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
KAGGLE_FAISS_DATASET_SLUG = "harshwardhanbhangale/netlingo-faiss-mitre-cve"
KAGGLE_UGR_DATASET_SLUG   = "harshwardhanbhangale/ugr-test"    # update if different

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 7)  Final results JSONL will be saved to Drive                     ‚îÇ
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
RESULTS_PATH_DRIVE = DRIVE_PROJ_PATH / "results" / "ugr16_inference_results.jsonl"
RESULTS_PATH_DRIVE.parent.mkdir(parents=True, exist_ok=True)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 8)  Summary print‚Äëout                                              ‚îÇ
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
summary = {
    "Drive project dir"     : str(DRIVE_PROJ_PATH),
    "Falcon adapter dir"    : str(ADAPTER_PATH_DRIVE),
    "FAISS local dir"       : str(LOCAL_FAISS_DIR),
    "UGR local dir"         : str(LOCAL_UGR_DIR),
    "UGR CSVs (expected)"   : [UGR_XTEST_FILE, UGR_YTEST_FILE],
    "FAISS Kaggle slug"     : KAGGLE_FAISS_DATASET_SLUG,
    "UGR   Kaggle slug"     : KAGGLE_UGR_DATASET_SLUG,
    "Results file"          : str(RESULTS_PATH_DRIVE),
}
print("\nCurrent path configuration:")
pprint(summary)

# quick existence checks (non‚Äëfatal warnings)
if not ADAPTER_PATH_DRIVE.exists():
    print("‚ö†Ô∏è  WARNING: Falcon LoRA adapter folder not found yet ‚Äì will check again later.")
if not UGR_XTEST_PATH.exists():
    print("‚ÑπÔ∏è  UGR CSVs not present ‚Äì they will be downloaded in Cell¬†4.")

# ==============================================================================
# Cell 4 ¬∑ Setup Kaggle API & download datasets   (patched shutil import)
# ==============================================================================
print("\n--- Cell 4: Setup Kaggle API & Download Datasets ---")

import os, pathlib, json, shutil           # ‚Üê added shutil here
from pathlib import Path

# --- >> IMPORTANT: Set your Kaggle username and dataset slugs << ---
KAGGLE_FAISS_DATASET_SLUG = "harshwardhanbhangale/netlingo-faiss-mitre-cve"   # ‚Üê change if needed
# KAGGLE_UGR_DATASET_SLUG was defined in Cell‚ÄØ3
# ------------------------------------------------------------------

# 0) Kaggle credentials --------------------------------------------------------
if not os.path.exists("/root/.kaggle/kaggle.json"):
    print("Upload your kaggle.json file:")
    from google.colab import files
    uploaded = files.upload()
    if "kaggle.json" not in uploaded:
        raise FileNotFoundError("‚ùå kaggle.json not uploaded.")
    os.makedirs("/root/.kaggle", exist_ok=True)
    shutil.copy("kaggle.json", "/root/.kaggle/")
    os.chmod("/root/.kaggle/kaggle.json", 0o600)
    print("‚úÖ Kaggle API configured.")
else:
    print("Kaggle API credentials already exist.")

# ------------------------------------------------------------------#
# 1) Download FAISS dataset                                          #
# ------------------------------------------------------------------#
print(f"\nDownloading FAISS dataset  ‚ûü  {KAGGLE_FAISS_DATASET_SLUG}")
cmd = f"kaggle datasets download -d {KAGGLE_FAISS_DATASET_SLUG} -p {str(LOCAL_FAISS_DIR)} --unzip --force"
print(f"‚Ü≥ {cmd}")
if os.system(cmd):
    raise RuntimeError("‚ùå Kaggle FAISS download failed")

# sanity‚Äëcheck
if not LOCAL_FAISS_INDEX_PATH.exists() or not LOCAL_ID_MAP_PATH.exists():
    raise FileNotFoundError("‚ùå FAISS files missing after download")
print(f"‚úÖ FAISS data ready  ‚Üí  {LOCAL_FAISS_DIR}")
!ls -lh "{LOCAL_FAISS_DIR}"

# ------------------------------------------------------------------#
# 2) Download UGR16 test dataset                                     #
# ------------------------------------------------------------------#
# ------------------------------------------------------------------
# Download UGR16 test dataset
# ------------------------------------------------------------------
print(f"\nDownloading UGR16 test dataset ‚ûü  {KAGGLE_UGR_DATASET_SLUG}")

cmd = (
    f"kaggle datasets download -d {KAGGLE_UGR_DATASET_SLUG} "
    f"-p {LOCAL_UGR_DIR} --unzip --force"
)
print("‚Ü≥", cmd)
exit_code = os.system(cmd)
if exit_code != 0:
    raise RuntimeError(f"Kaggle UGR download failed (exit {exit_code}).")

# expected CSVs (update names if you use another variant)
UGR_XTEST_PATH = LOCAL_UGR_DIR / "UGR16v2_Xtest.csv"
UGR_YTEST_PATH = LOCAL_UGR_DIR / "UGR16v2_Ytest.csv"

if not UGR_XTEST_PATH.exists() or not UGR_YTEST_PATH.exists():
    print("‚ùå Expected UGR files not found:")
    !ls -lh {LOCAL_UGR_DIR}
    raise FileNotFoundError("UGR test CSVs missing after download.")
else:
    print("‚úÖ UGR test data ready  ‚Üí", LOCAL_UGR_DIR)
    !ls -lh {LOCAL_UGR_DIR}

# ==============================================================================
# Cell‚ÄØ5 ¬∑ Load models & artefacts  (vision‚Äëfree)
# ==============================================================================

print("\n--- Cell‚ÄØ5: Loading models & artifacts ---")

import torch, json, faiss, types, sys
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
from sentence_transformers import SentenceTransformer

# ---------- 0) basic paths ----------------------------------------------------
ADAPTER_PATH_DRIVE   = Path("/content/drive/MyDrive/NetLingo_project/models/netlingo_falcon_lora_adapters")
LOCAL_FAISS_INDEX    = Path("/content/faiss_data/faiss_mitre_cve_index.bin")
LOCAL_FAISS_IDMAP    = Path("/content/faiss_data/faiss_mitre_cve_id_map.json")

# ---------- 1) Falcon base ----------------------------------------------------
bnb_cfg = BitsAndBytesConfig(
    load_in_4bit           = True,
    bnb_4bit_quant_type    = "nf4",
    bnb_4bit_compute_dtype = torch.bfloat16,
)

print("‚Ä¢ Loading Falcon base ‚Ä¶")
base_model = "tiiuae/falcon-rw-1b"
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    device_map="auto",
    trust_remote_code=True,
    quantization_config=bnb_cfg,
)

# ---------- 2) Tokenizer (matching LoRA) -------------------------------------
print("‚Ä¢ Loading tokenizer stored with adapters ‚Üí", ADAPTER_PATH_DRIVE)
tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH_DRIVE)
model.resize_token_embeddings(len(tokenizer))   # ensures same vocab size

# ---------- 3) Attach LoRA ----------------------------------------------------
print("‚Ä¢ Loading LoRA adapters   ‚Üí", ADAPTER_PATH_DRIVE)
model = PeftModel.from_pretrained(model, ADAPTER_PATH_DRIVE)
print("‚Ä¢ Merging LoRA weights ‚Ä¶")
model = model.merge_and_unload()                # now a plain nn.Module
print("‚úî LoRA merged")

# ---------- 4) Add runtime‚Äëspecific token -------------------------------------
new_tok = tokenizer.add_tokens(["DS_UGR"])
if new_tok:
    model.resize_token_embeddings(len(tokenizer))
    print(f"‚Ä¢ Added {new_tok} extra token(s) ‚Äì new vocab:", len(tokenizer))

# ---------- 5) FAISS ----------------------------------------------------------
print("‚Ä¢ Loading FAISS index ‚Ä¶")
faiss_index = faiss.read_index(str(LOCAL_FAISS_INDEX))
with open(LOCAL_FAISS_IDMAP, "r") as f:
    id_map = {int(k): v for k, v in json.load(f).items()}
print("‚úî FAISS index loaded  (vectors:", faiss_index.ntotal, ")")

# ---------- 6) Sentence‚ÄëTransformer¬†(‚ö†¬†imports after timm‚Äëstub!) -------------
print("‚Ä¢ Loading Sentence‚ÄëTransformer (embeddings) ‚Ä¶")
st_model = SentenceTransformer("all-MiniLM-L6-v2")   # <‚Äë‚Äë now imports safely
print("‚úî Sentence‚ÄëTransformer ready")

print("\n‚úÖ  All artefacts loaded ‚Äì you're good to continue.")

# ==============================================================================
# Cell‚ÄØ6 ¬∑ Setup Google‚ÄØGemini API key
# ==============================================================================
print("\n--- Cell 6: Setup Google Gemini API Key ---")

genai_model_client = None

try:
    # 1) Colab secrets first
    from google.colab import userdata
    gemini_api_key = userdata.get("GOOGLE_API_KEY")
    if gemini_api_key:
        print("‚úÖ Gemini API key loaded from Colab secrets.")
    else:
        print("üîë Gemini API key not found in Colab secrets ‚Äì please paste it.")
except ImportError:
    # not running inside Colab
    gemini_api_key = None

# 2) Fallback ‚Üí prompt user
if not gemini_api_key:
    import getpass
    gemini_api_key = getpass.getpass("Please enter your Google‚ÄØGemini API key: ")

# 3) Configure the client (if a key was provided)
if gemini_api_key:
    try:
        genai.configure(api_key=gemini_api_key)
        genai_model_client = genai.GenerativeModel("gemini-2.0-flash-latest")
        print("‚úÖ Google‚ÄØGemini client initialised (gemini‚Äë2.0‚Äëflash‚Äëlatest).")
    except Exception as e:
        print(f"‚ùå Could not initialise Gemini client ‚Äì {e!s}")
else:
    print("‚ùå No Gemini API key provided ‚Äì explanation generation will be skipped.")

# ==============================================================================
# Cell‚ÄØ7 ¬∑ Helper functions (UGR‚Äëspecific tokeniser, inference, FAISS, Gemini)
# ==============================================================================
print("\n--- Cell‚ÄØ7: Defining helper functions ---")

# ‚îÄ‚îÄ 1)  Column map & bucketing rules  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
UGR_COLUMN_MAP = {
    "td"   : "DUR_",     # duration   (sec)
    "sa"   : "SRCIP_",   # src IP
    "da"   : "DSTIP_",   # dst IP
    "sp"   : "SPORT_",   # src port
    "dp"   : "DPORT_",   # dst port
    "pr"   : "PROTO_",   # protocol
    "flg"  : "FLAGS_",   # TCP flags
    "pkt"  : "PKTS_",    # packets
    "byt"  : "BYTES_",   # bytes
    #
    "binary_label": "LABEL_COL_UGR",     # will be **added later** (ground‚Äëtruth)
}
UGR_MAP_FILTERED = {k: v for k, v in UGR_COLUMN_MAP.items()
                    if k != "binary_label"}          # no label in sentence
UGR_LABEL_COL = "binary_label"

NUMERIC_COLS_UGR = ["td", "pkt", "byt"]

BUCKETS_UGR = {
    "td":  [(0, 1, "0-1s"), (1, 10, "1-10s"), (10, 60, "10-60s"),
            (60, 300, "1m-5m"), (300, np.inf, ">5m")],
    "pkt": [(0, 5, "0-5"), (5, 50, "5-50"), (50, 500, "50-500"),
            (500, 5_000, "500-5K"), (5_000, np.inf, ">5K")],
    "byt": [(0,   100, "0-100B"), (100, 1_000, "100B-1K"),
            (1_000, 100_000, "1K-100K"), (100_000, 10_000_000, "100K-10M"),
            (10_000_000, np.inf, ">10M")],
}

def _bucket_numeric(val: float, rules):
    """Return the bucket label for *val* according to *rules* list[(low, high, lbl)]"""
    for low, high, lbl in rules:
        if low <= val < high:
            return lbl
    return rules[-1][2]         # ‚â• last threshold ‚Üí last label

def bucket_value_generic(val, col_name):
    """Convert raw value -> bucket / cleaned token‚Äëready string."""
    if pd.isna(val) or str(val).strip() == "":
        return None

    # Special‚Äëcase infinities / non‚Äëfinite numbers
    try:
        if isinstance(val, str) and "inf" in val.lower():
            return "INF"
    except Exception:
        pass

    # Numeric bucketing
    if col_name in BUCKETS_UGR and isinstance(val, (int, float, np.number)):
        try:
            num = float(val)
            if not np.isfinite(num):
                return f"NonFinite_{col_name}"
            return _bucket_numeric(num, BUCKETS_UGR[col_name])
        except Exception:
            return f"NaN_{col_name}"

    # Otherwise: strip naughty chars & upper‚Äëcase
    return re.sub(r"[^A-Za-z0-9]", "", str(val).upper())

# ‚îÄ‚îÄ 2)  Tokenise one UGR row ---------------------------------------------------
def tokenize_ugr_test_flow(row: pd.Series,
                           cmap=UGR_MAP_FILTERED,
                           tag="DS_UGR") -> str:
    """Turn a Pandas row into a space‚Äëseparated token sentence."""
    toks = [tag]
    for col, prefix in cmap.items():
        if col not in row:
            continue
        token_val = bucket_value_generic(row[col], col)
        if token_val:
            toks.append(f"{prefix}{token_val}")
    if len(toks) == 1:      # nothing added ‚áí fallback token so model doesn‚Äôt crash
        toks.append("EMPTY")
    return " ".join(toks)

# ‚îÄ‚îÄ 3)  Inference with the fine‚Äëtuned Falcon‚ÄëLoRA -----------------------------
def get_model_verdict(token_sentence: str, max_length: int = 512):
    """Return ('ATTACK' | 'BENIGN', confidence‚àà[0,1], err_msg_or_None)"""
    try:
        # encode on the model‚Äôs device
        inputs = tokenizer(token_sentence, return_tensors="pt",
                           truncation=True, max_length=max_length).to(model.device)

        # need the ids of the special label tokens
        id_ok  = tokenizer.convert_tokens_to_ids("LABEL_BENIGN")
        id_bad = tokenizer.convert_tokens_to_ids("LABEL_ATTACK")
        if id_ok is None or id_bad is None:
            return "ERROR", 0.0, "label tokens missing from vocab"

        with torch.no_grad():
            out = model.generate(**inputs,
                                 max_new_tokens=1,
                                 pad_token_id=tokenizer.pad_token_id,
                                 eos_token_id=tokenizer.eos_token_id,
                                 output_scores=True,
                                 return_dict_in_generate=True)

        logits = out.scores[0][0]          # first token logits
        probs  = torch.softmax(logits, dim=-1)
        p_bad, p_ok = probs[id_bad].item(), probs[id_ok].item()
        norm = max(p_bad + p_ok, 1e-6)
        if p_bad > p_ok:
            return "ATTACK", p_bad / norm, None
        else:
            return "BENIGN", p_ok / norm, None
    except Exception as e:
        return "ERROR", 0.0, str(e)

# ‚îÄ‚îÄ 4)  FAISS nearest‚Äëneighbour lookup ---------------------------------------
def get_faiss_context(sentence: str, k: int = 3):
    if st_model is None:
        return [], []
    try:
        emb = st_model.encode([sentence]).astype(np.float32)
        dists, idxs = faiss_index.search(emb, k)
        return [faiss_id_map.get(int(i), "UNK") for i in idxs[0]], dists[0]
    except Exception as e:
        print(f"‚ùóÔ∏è FAISS error: {e}")
        return [], []

# ‚îÄ‚îÄ 5)  Token ‚Üí human summary (for Gemini prompt) ----------------------------
TOKEN_TRANSLATION_MAP = {
    "PROTO_TCP": "Protocol:‚ÄØTCP",
    "PROTO_UDP": "Protocol:‚ÄØUDP",
    "DPORT_80":  "Destination port‚ÄØ80¬†(HTTP)",
    "DPORT_443": "Destination port‚ÄØ443¬†(HTTPS)",
    "DPORT_22":  "Destination port‚ÄØ22¬†(SSH)",
    "DPORT_3389":"Destination port‚ÄØ3389¬†(RDP)",
    "FLAGS_S":   "TCP flag‚ÄØSYN",
    "FLAGS_FPA": "TCP flags‚ÄØFIN‚ÄëPSH‚ÄëACK",
    "DUR_0-1s":  "Duration <‚ÄØ1‚ÄØs",
    "DUR_>5m":   "Duration >‚ÄØ5‚ÄØmin",
    "BYTES_0-100B": "Payload <‚ÄØ100‚ÄØB",
    "BYTES_>10M":   "Payload >‚ÄØ10‚ÄØMB",
    "PKTS_0-5":     "Packet count <‚ÄØ5",
    "PKTS_>5K":     "Packet count >‚ÄØ5‚ÄØk",
}
def translate_tokens_for_prompt(token_sentence: str) -> str:
    parts = []
    for tok in token_sentence.split():
        if any(tok.startswith(p) for p in
               ("SRCIP_", "DSTIP_", "SPORT_", "DPORT_", "PROTO_", "FLAGS_",
                "DUR_", "BYTES_", "PKTS_")):
            parts.append("- " + TOKEN_TRANSLATION_MAP.get(tok, tok.replace("_", " ")))
    return "\n".join(parts[:15]) if parts else "- (no salient tokens)"

# ‚îÄ‚îÄ 6)  Call Gemini -----------------------------------------------------------
def generate_gemini_explanation(flow_summary: str, faiss_ctx_text: str):
    if genai_model_client is None:
        return "Gemini client not initialised."
    prompt = f"""
Analyze the following network flow and the retrieved threat‚Äëintel context.
Explain concisely why it might be malicious and, if possible, map it to a
MITRE‚ÄØATT&CK tactic/technique.

Network flow:
{flow_summary}

Context (nearest MITRE/CVE snippets):
{faiss_ctx_text}

Analysis:"""
    try:
        cfg = genai.types.GenerationConfig(max_output_tokens=160, temperature=0.3)
        safety = [{"category": c, "threshold": "BLOCK_MEDIUM_AND_ABOVE"}
                  for c in ["HARM_CATEGORY_HARASSMENT",
                            "HARM_CATEGORY_HATE_SPEECH",
                            "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                            "HARM_CATEGORY_DANGEROUS_CONTENT"]]
        resp = genai_model_client.generate_content(prompt,
                                                   generation_config=cfg,
                                                   safety_settings=safety)
        return resp.text.strip() if resp.parts else "(response blocked)"
    except Exception as e:
        return f"Gemini error: {e}"

print("‚úÖ Helper functions ready.")

# ==============================================================================
# Cell 7 ¬∑ Helper functions  (unchanged + tiny tweaks)
# ==============================================================================
print("\n--- Cell¬†7: (re)defining helper functions ---")

# [‚Ä¶]  ‚Äì‚Äì keep everything you already had ‚Äì‚Äì
# only TWO small additions shown below ‚Üì‚Üì‚Üì

# ------------------------------------------------------------------
#  A small util: if X‚Äëtest already has a ready‚Äëmade `token_sentence`
#  column, we just use it instead of re‚Äëtokenising row‚Äëby‚Äërow
# ------------------------------------------------------------------
def ensure_token_sentence(df):
    if "token_sentence" in df.columns:
        return df["token_sentence"].astype(str)
    # else fall back to row‚Äëwise NetLingo tokenisation
    return df.apply(tokenize_ugr_test_flow, axis=1)

# Slight tweak to generate a short ID for each dataset in results
def make_result_id(ds_name, row_idx):
    return f"{ds_name}#{row_idx}"

print("‚úÖ¬†Helper functions ready.")

# ==============================================================================
# Cell 8 ¬∑ Run inference on Kyoto / UNSW / CIC subsets  (max‚Äë1000 each)
# ==============================================================================
print("\n--- Cell¬†8: Inference on Kyoto / UNSW / CIC (max‚Äë1000 rows each) ---")

import pathlib, json, gc
from tqdm.auto import tqdm

SUBSET_DIR              = pathlib.Path("/content/drive/MyDrive/NetLingo_project/data/testing_subsets")
ROWS_PER_DATASET        = 1_000        # ‚Üê hard cap
CONFIDENCE_THRESHOLD    = 0.80
FAISS_K                 = 3

DATASETS = {
    "Kyoto": {"x": SUBSET_DIR/"Kyoto_Xtest_50000.csv",  "y": SUBSET_DIR/"Kyoto_Ytest_50000.csv"},
    "UNSW":  {"x": SUBSET_DIR/"UNSW_Xtest.csv",         "y": SUBSET_DIR/"UNSW_Ytest.csv"},
    "CIC":   {"x": SUBSET_DIR/"CIC_Xtest.csv",          "y": SUBSET_DIR/"CIC_Ytest.csv"},
}

OUT_AGG = SUBSET_DIR / "NetLingo_all_results.jsonl"
OUT_AGG.unlink(missing_ok=True)   # reset

def ensure_binary_label(df_y):
    if UGR_LABEL_COL in df_y.columns:
        return df_y[UGR_LABEL_COL].astype(int)
    # otherwise derive: any attack count >‚ÄØ0 ‚Üí 1
    return (df_y.sum(axis=1) > 0).astype(int)

for name, paths in DATASETS.items():
    x_path, y_path = paths["x"], paths["y"]
    if not (x_path.exists() and y_path.exists()):
        print(f"‚ö†Ô∏è  {name} skipped ‚Äì files missing.")
        continue

    print(f"\n‚ñ∂ {name}  ({ROWS_PER_DATASET} rows)")
    df_X = pd.read_csv(x_path, nrows=ROWS_PER_DATASET, low_memory=False)
    df_y = pd.read_csv(y_path, nrows=ROWS_PER_DATASET, low_memory=False)
    df_y[UGR_LABEL_COL] = ensure_binary_label(df_y)

    token_col = ensure_token_sentence(df_X)
    results   = []

    for idx, token_sentence in tqdm(enumerate(token_col),
                                    total=len(token_col),
                                    desc=f"{name} infer"):
        verdict, conf, err = get_model_verdict(token_sentence)
        if err:
            continue
        gt        = "ATTACK" if df_y.iloc[idx][UGR_LABEL_COL] == 1 else "BENIGN"
        correct   = verdict == gt

        faiss_res, expl = [], "N/A"
        if verdict == "ATTACK" and conf >= CONFIDENCE_THRESHOLD:
            ids, dists = get_faiss_context(token_sentence, k=FAISS_K)
            faiss_res  = list(zip(ids, dists.tolist()))
            expl       = generate_gemini_explanation(
                            translate_tokens_for_prompt(token_sentence),
                            json.dumps(faiss_res))

        results.append({
            "id":          f"{name}#{idx}",
            "dataset":     name,
            "token_sentence": token_sentence,
            "ground_truth": gt,
            "prediction":   verdict,
            "confidence":   round(conf, 4),
            "correct":      correct,
            "faiss":        faiss_res,
            "explanation":  expl,
        })

    out_file = SUBSET_DIR / f"{name}_results.jsonl"
    with open(out_file, "w", encoding="utf-8") as f:
        for r in results:
            f.write(json.dumps(r) + "\n")
    print(f"‚úî Saved {len(results)} rows ‚Üí {out_file.name}")

    # append to aggregate
    with open(OUT_AGG, "a", encoding="utf-8") as agg:
        for r in results:
            agg.write(json.dumps(r) + "\n")

    del df_X, df_y, results
    gc.collect()

print("\nAll datasets processed (max 1‚ÄØ000 rows each).")
print(f"Aggregate file ‚Üí {OUT_AGG}")

